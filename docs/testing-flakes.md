# OpenShift on Azure testing flakes triaging

A lot of end-to-end / conformance tests depend on resources that may vary in
time with different workloads, software stack versions and available
hardware. For that reason, a reasonable chunk of our test harness isn't deterministic.
A side effect of that is that new Pull Requests advancing through the testing pipeline
may fail with reasons that are unrelated to the changes that comprises that pull request
but rather by some underlying components that the cluster uses to run these tests.
Those test failures are called flakes.

The following is intended as a guide for identifying those testing flakes and to
document them as well as possible so that it's possible to come back at a later
time and help eliminate them or lower their occurrence rate.

## What to do when a PR testing fails

### General guidelines

* If you can fix the flake, do so.
* If you can't fix the flake, add judicious logging or saving of artifacts to try to make
  the flake fixable next time around.
* Fixing flakes is everyone's responsibility.

### Checklist in order of execution

1. Make sure the error isn't related to the change. When confirmed, it's time to investigate
   the flake further.
2. After initial inspection of the flake (See Tricks below), either :
	* CREATE a new github issue to report a flake never seen before. Add some important
      tracking info:
    	* A brief but complete description of how the flake happened with the last
          or most relevant error message in the **Description** field is **key** for other
          developers to be able to identify an already reported flake.
    	* Include a **Comment** to that issue with links to PR(s) where the
    	  flake happened and to the CI-operator diagnostic page for the failing test.
	* **OR** UPDATE with additional findings a previously created issue for that flake:
		* **CARE** - you want to be sure the flake matches - if not, open a new one.
		* A new **Comment** with a link to the log where the flake happened.
	* Nice to have:
		* Variations in errors messages from where it was encountered before.
		* Complete or partial tracing of to the source of the problem - if you started to
		  work on fixing the flake.

3. Ensure the flake issue is labeled appropriately:
```
/kind test-flake
```

4. Back to your PR page, try to get passed the testing failure:
```
/retest               # to retry failed tests
```

good luck!

## Tricks to work with flakes

* Take a look at the docs/testing.md file for general testing documentation.
* When a CI test fails look at the CI-operator issued report in the PR discussion and click
  on the "link" link on the test that failed and you will be taken to the CI-operator PR
  diagnostic page. There's 2 links that are interesting to click on to get started with debugging.
	* build log: This displays the complete build and execution log for the tests. This log
          will include the error messages of what might have went wrong during testing.
	* artifacts: This section includes all data files and logs that were generated by different
          portions of the code and agglomerated in a predefined storage folder ($ARTIFACT_DIR -- to
          be renamed $ARTIFACTS soon). You will find information about the state of :
		* pods
		* events
		* controller manager
* In complex situations, it is possible to add custom data to the artifact directory for later
  harvesting. Look at ./hack/artifacts.sh.
* At the moment, for lightweight debugging, add Geneva log entries. For heavyweight debugging where
  artifacts are required, use artifacts. Be judicious either way.
* Currently, some logs are stored as artifacts but are also available in Geneva. Also some artifacts
  are a crutch because actually log lines are sufficient to diagnose. We will reduce artifacts over
  time, and it is recommended to use Geneva logging where applicable.